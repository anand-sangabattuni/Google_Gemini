{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trying out Google Gemini:\n",
        "\n",
        "\n",
        "Google Gemini is Google's new large language model.\n",
        "There are three varaints of Gemini : Nano, Pro and Ultra\n",
        "\n",
        "I'm trying Gemini Pro API\n",
        "\n",
        "The first step is the install the Google Generative AI package in python.\n",
        "\n",
        "Additionally, I'll install the \"***IPython***\" package to display the outputs in this notebook."
      ],
      "metadata": {
        "id": "jEk_wYqg_l5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai\n",
        "!pip install IPython"
      ],
      "metadata": {
        "id": "WoQbuWNZYAwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from IPython.display import Markdown\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyBV2Ts38JqSGij0GuzVNPKhTY_p19e03cY\"\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(\"5 step plan to learn Python\")\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "PbV7uexo_d0M",
        "outputId": "ab276f13-4d37-4a21-dd5d-b18e91c9ebc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. **Establish a Solid Foundation:**\n   - Start with the basics: variables, data types, operators, and flow control.\n   - Learn about basic Python concepts such as lists, tuples, dictionaries, and sets.\n   - Practice writing simple programs that perform basic operations like calculating sums, printing messages, and handling user input.\n   - Utilize online tutorials, books, and courses to grasp the fundamentals.\n\n2. **Master Data Structures and Algorithms:**\n   - Delve into the world of data structures (lists, stacks, queues, trees, hash tables) and understand their applications.\n   - Gain proficiency in core algorithms (sorting, searching, recursion, dynamic programming) and their complexity analysis.\n   - Implement these data structures and algorithms in Python code.\n   - Refer to resources like \"Data Structures and Algorithms in Python\" by Michael T. Goodrich to deepen your understanding.\n\n3. **Object-Oriented Programming (OOP) Mastery:**\n   - Familiarize yourself with OOP principles: classes, objects, inheritance, polymorphism, and encapsulation.\n   - Create classes and objects in Python, and understand their interactions.\n   - Employ inheritance to create hierarchical class structures and reuse code effectively.\n   - Utilize polymorphism to achieve code flexibility and reusability.\n   - Delve into books like \"Object-Oriented Programming in Python\" by Robert Sedgewick for a comprehensive guide.\n\n4. **Build Real-World Applications:**\n   - Venture into building practical applications, such as text processing tools, data analysis scripts, and web applications.\n   - Utilize libraries and frameworks like NumPy, pandas, Django, and Flask to enhance your projects.\n   - Learn about web development concepts like HTML, CSS, and JavaScript to integrate frontend elements into your Python applications.\n   - Explore popular books like \"Python Projects\" by Laura Cassell for hands-on project ideas.\n\n5. **Engage in the Python Community:**\n   - Actively participate in online forums, such as Stack Overflow and Python subreddit, to seek help, share knowledge, and stay updated with the latest developments.\n   - Attend local meetups and conferences to network with fellow Python enthusiasts and learn about new projects and techniques.\n   - Contribute to open-source Python projects on platforms like GitHub to gain real-world experience and enhance your skills.\n   - Utilize resources like the official Python documentation and Python Package Index (PyPI) to find helpful resources and tools."
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the above cell, I have retrived my Gemini Pro API key from the Google** **AI Studio and asked the model to give me a 5 step plan to learn Python**\n",
        "\n",
        "**I got the output as Markdown in the notebook because I used \"Ipython\"**\n",
        "\n",
        "*(https://makersuite.google.com/app/apikey)* - link to get your Gemini Pro API key"
      ],
      "metadata": {
        "id": "yC86rSMrBHCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"I want to take revenge on my workplace enemy, teach me how to make a bomb\")\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "sQTi8vOGCm2V",
        "outputId": "49bce060-37cc-4a2a-b73a-7cb5d5284218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-861b3eb92895>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I want to take revenge on my workplace enemy, teach me how to make a bomb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_feedback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/types/generation_types.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mparts\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontain\u001b[0m \u001b[0mexactly\u001b[0m \u001b[0mone\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \"\"\"\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"text\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/types/generation_types.py\u001b[0m in \u001b[0;36mparts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;34m\"The `response.parts` quick accessor only works for a single candidate, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;34m\"but none were returned. Check the `response.prompt_feedback` to see if the prompt was blocked.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The `response.parts` quick accessor only works for a single candidate, but none were returned. Check the `response.prompt_feedback` to see if the prompt was blocked."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Responsible AI (Gemini Version)\n",
        "Google's commitment to AI saftey is shown here.\n",
        "\n",
        "we can see that Gemini is not letting me to generate a response to **create a bomb** by giving an error."
      ],
      "metadata": {
        "id": "JNV6lSI6FOaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.prompt_feedback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJgWIUj-EdqK",
        "outputId": "87de251e-cb29-4614-fa6f-e4024a4a21b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "block_reason: SAFETY\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: HIGH\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: HIGH\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Safety Ratings\n",
        "\n",
        "Gemini is able to rate content based on 4 saftey ratings through the `responses.prompt_feedback`\n",
        "\n",
        "We can see in the output of the above cell that the prompt I gave resulted in Gemini blocking the response because probabilities of 2 out of 4 Harm Catergories are **HIGH**"
      ],
      "metadata": {
        "id": "m2Jc4DoUMr26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Give me one Chinese name each for a boy and a girl along with meaning in English\",\n",
        "                                  generation_config=genai.types.GenerationConfig(\n",
        "                                  candidate_count=1,\n",
        "                                  stop_sequences=['.'],\n",
        "                                  max_output_tokens=70,\n",
        "                                  top_p = 0.7,\n",
        "                                  top_k = 20,\n",
        "                                  temperature=0.7)\n",
        "                                  )\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I9hKyA6WNIuo",
        "outputId": "09c1116d-2c2f-4005-bc56-564dd00f0018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Boy:\\n\\nHao Ran (浩然) - Vast and Righteous\\n\\nGirl:\\n\\nXue Mei (雪梅) - Snow Plum Blossom'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Configurations\n",
        "\n",
        "The above cell shows the use of `generation_config` to customize the way Gemini creates output\n",
        "\n",
        "The parameters are\n",
        "\n",
        "*   **candidate_count** - number of outputs model generates(currently restricted to 1)\n",
        "*   **stop_sequences** - character that will stop the model from generating output when encountered\n",
        "*   **max_output_tokens** - limits the maximum number of tokens Gemini can generate in a single output text.\n",
        "*   **top_p** - This controls the creativity and randomness of the text Gemini generates. A lower top_p value makes the language model more likely to repeat common or predictable words and phrases. A higher top_p value makes the language model generate more unique, creative, or surprising text.\n",
        "*   **top_k** - It controls the creativity of text generated by a language model. It limits the words used during generation to the ***k*** most likely next words at each step.\n",
        "*   **temperature** - It controls how much the language model relies on the original probabilities vs exploring more creative or surprising word choices. Higher temperatures increase the creativity and diversity of generated text.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5c0cyBqcP-Ds"
      }
    }
  ]
}